{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOBBcSBaH+eJ7CtU2HDFhLl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 3.2 Capturing data dependencies with attetion mechanisms\n","\n","Mecanismo de atencion de Badanau RNN inspiro para el desarollo del mecnaismo de attencion para la arquitectura de los transformers."],"metadata":{"id":"S0IRhGp90lXj"}},{"cell_type":"markdown","source":[],"metadata":{"id":"Xqe4KyRj2x4J"}},{"cell_type":"code","execution_count":90,"metadata":{"id":"oNHjtVnGl18c","executionInfo":{"status":"ok","timestamp":1761017507318,"user_tz":180,"elapsed":31,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}}},"outputs":[],"source":["import torch"]},{"cell_type":"markdown","source":["### 3.3.1 Mecanismo de autoatencion sin pesos entrenables"],"metadata":{"id":"wQRkiKVsgBwQ"}},{"cell_type":"code","source":["inputs = torch.tensor(\n","  [[0.43, 0.15, 0.89], # Your     (x^1)\n","   [0.55, 0.87, 0.66], # journey  (x^2)\n","   [0.57, 0.85, 0.64], # starts   (x^3)\n","   [0.22, 0.58, 0.33], # with     (x^4)\n","   [0.77, 0.25, 0.10], # one      (x^5)\n","   [0.05, 0.80, 0.55]] # step     (x^6)\n",")"],"metadata":{"id":"myAt48ZnPBJz","executionInfo":{"status":"ok","timestamp":1761017507330,"user_tz":180,"elapsed":6,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}}},"execution_count":91,"outputs":[]},{"cell_type":"code","source":["query = inputs[1]         #El segundo token de entrada actúa como la consulta (query).\n","attn_scores_2 = torch.empty(inputs.shape[0])\n","for i, x_i in enumerate(inputs):\n","  attn_scores_2[i]=torch.dot(x_i, query)\n","print(attn_scores_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nO0xFxBtQ9r8","executionInfo":{"status":"ok","timestamp":1761017507360,"user_tz":180,"elapsed":32,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"35992f14-83f2-40c7-c9fc-a6e9a9e773b4"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"]}]},{"cell_type":"code","source":["# Un metodo de normalizacion la sumatoria de la\n","# normalizacion debe ser 1\n","attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n","print('Pesos de Atencion: ', attn_weights_2_tmp)\n","print('Sum: ', attn_weights_2_tmp.sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_foNYw92RrnC","executionInfo":{"status":"ok","timestamp":1761017507385,"user_tz":180,"elapsed":20,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"9b684cc5-aa2e-49fd-e852-f1d293d7cac7"},"execution_count":93,"outputs":[{"output_type":"stream","name":"stdout","text":["Pesos de Atencion:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n","Sum:  tensor(1.0000)\n"]}]},{"cell_type":"code","source":["# Softmax : es mejor para manejar valores extremos y\n","#ofrece propiedades de gradiente más favorables durante el entrenamiento\n","\n","def softmax_naive(x):\n","  return torch.exp(x) / torch.exp(x).sum(dim=0)\n","\n","attn_weights_2_naive = softmax_naive(attn_scores_2)\n","print('Pesos de Atencion: ', attn_weights_2_naive)\n","print('Sum: ', attn_weights_2_naive.sum())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_zveBD1IWRwk","executionInfo":{"status":"ok","timestamp":1761017507450,"user_tz":180,"elapsed":60,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"a72ec6e2-752d-4f28-89f7-b1950b91751f"},"execution_count":94,"outputs":[{"output_type":"stream","name":"stdout","text":["Pesos de Atencion:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n","Sum:  tensor(1.)\n"]}]},{"cell_type":"code","source":["#(softmax_naive) puede enfrentar problemas de inestabilidad numérica,\n","#como desbordamiento (overflow) o subdesbordamiento (underflow)\n","# al trabajar con valores de entrada muy grandes o muy pequeños.\n","# por eso se recomienda utilizar la implementación de PyTorch, que ha sido\n","# optimizada para rendimiento y estabilidad:\n","\n","attn_weights_2 = torch.softmax(attn_scores_2, dim =0)\n","print('Pesos de Atención: ', attn_weights_2)\n","print('Sum: ', attn_weights_2.sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uSfMpS05bTLi","executionInfo":{"status":"ok","timestamp":1761017507491,"user_tz":180,"elapsed":39,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"cb1b2308-6384-4b56-d1ce-2f9e13984ac2"},"execution_count":95,"outputs":[{"output_type":"stream","name":"stdout","text":["Pesos de Atención:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n","Sum:  tensor(1.)\n"]}]},{"cell_type":"code","source":["# calcular el vector de contexto\n","query = inputs[1]\n","context_vec_2 = torch.zeros(query.shape)\n","for i, x_i in enumerate(inputs):\n","  context_vec_2 += attn_weights_2[i]*x_i\n","print(context_vec_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Kc3VUChdCMP","executionInfo":{"status":"ok","timestamp":1761017507494,"user_tz":180,"elapsed":26,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"55053acf-1b91-498e-d3fd-234bb87d7fb5"},"execution_count":96,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.4419, 0.6515, 0.5683])\n"]}]},{"cell_type":"markdown","source":["### 3.3.2 Computing attention weights for all input tokens"],"metadata":{"id":"nsQzREYggUcS"}},{"cell_type":"markdown","source":["#### Paso 1. Calcula los puntajes de atención como productos punto entre las entradas."],"metadata":{"id":"bJQjQ2OEquUW"}},{"cell_type":"code","source":["attn_scores = torch.empty(6, 6)\n","for i, x_i in enumerate(inputs):\n","  for j, x_j in enumerate(inputs):\n","    attn_scores[i,j]=torch.dot(x_i, x_j)\n","\n","print(attn_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VRZVcgVAgTfk","executionInfo":{"status":"ok","timestamp":1761017507526,"user_tz":180,"elapsed":30,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"bd44a955-3335-4a54-8894-33b4fe52e5ec"},"execution_count":97,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n","        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n","        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n","        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n","        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n","        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"]}]},{"cell_type":"code","source":["# Mismos resultado usando multiplicacionde matriz\n","# los for loops en python son lentos\n","attn_scores= inputs @ inputs.T\n","print(attn_scores)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"crt3Lf4_pRL6","executionInfo":{"status":"ok","timestamp":1761017507593,"user_tz":180,"elapsed":63,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"f51ec695-ae08-4f8f-d923-7c0fb5ad8d67"},"execution_count":98,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n","        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n","        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n","        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n","        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n","        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"]}]},{"cell_type":"markdown","source":["### Paso 2. Normalizamos cada fila para que los valores en cada una sumen 1:\n"],"metadata":{"id":"QAEg5ziDq-c3"}},{"cell_type":"code","source":["# dim= -1 1indicando que la función softmax debe aplicar la normalización a lo largo de\n","# la última dimensión del tensor attn_scores\n","\n","attn_weigths = torch.softmax(attn_scores, dim=-1)\n","print(attn_weigths)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1d4nLZm-rQqF","executionInfo":{"status":"ok","timestamp":1761017507612,"user_tz":180,"elapsed":16,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"6a4e745e-56d5-4ff4-d56b-84d8ea0b381c"},"execution_count":99,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n","        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n","        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n","        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n","        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n","        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"]}]},{"cell_type":"code","source":["# verificando que todas las filas sumen 1\n","row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n","print(\"Fila 2 sum: \", row_2_sum)\n","print(\"Todas las filas sum: \", attn_weigths.sum(dim=-1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pyhnz_1OsLIj","executionInfo":{"status":"ok","timestamp":1761017507639,"user_tz":180,"elapsed":23,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"f206691c-33e8-41b1-a104-8e25dc115737"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stdout","text":["Fila 2 sum:  1.0\n","Todas las filas sum:  tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"]}]},{"cell_type":"markdown","source":["### Paso 3. usamos multiplicación de matrices para calcular todos los vectores de contexto de manera eficiente\n"],"metadata":{"id":"pjALva0Ets1a"}},{"cell_type":"code","source":["all_context_vecs = attn_weigths @ inputs\n","print(all_context_vecs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cj2arJIXt890","executionInfo":{"status":"ok","timestamp":1761017507664,"user_tz":180,"elapsed":20,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"08dd51af-6cee-4884-f23b-3ea593849e41"},"execution_count":101,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.4421, 0.5931, 0.5790],\n","        [0.4419, 0.6515, 0.5683],\n","        [0.4431, 0.6496, 0.5671],\n","        [0.4304, 0.6298, 0.5510],\n","        [0.4671, 0.5910, 0.5266],\n","        [0.4177, 0.6503, 0.5645]])\n"]}]},{"cell_type":"code","source":["print(\"Previous 2nd context vector:\", context_vec_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iNZV4Q5zuPlR","executionInfo":{"status":"ok","timestamp":1761017507684,"user_tz":180,"elapsed":16,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"7381e213-ee55-4778-9e62-0339ef269829"},"execution_count":102,"outputs":[{"output_type":"stream","name":"stdout","text":["Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"]}]},{"cell_type":"markdown","source":["## 3.4 Implementando autoatención con pesos entrenables."],"metadata":{"id":"GafIu5EOvVqB"}},{"cell_type":"code","source":["# Comenzamos aquí calculando solo un vector de contexto,\n","# z^{(2)}, con fines ilustrativos.\n","\n","x_2 = inputs[1]                 # Segundo elemento de entrada\n","d_in = inputs.shape[1]          # Tamaño de entrada del Embedding, d= 3\n","d_out = 2                       # Tamaño del embedding de salida"],"metadata":{"id":"mpcO1JcvvdHJ","executionInfo":{"status":"ok","timestamp":1761017507748,"user_tz":180,"elapsed":60,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}}},"execution_count":103,"outputs":[]},{"cell_type":"code","source":["# Inicializamos las tres matrices de pesos Wq, Wk y Wv\n","torch.manual_seed(123)\n","W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n","W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n","W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n","# Establecemos requires_grad=False para reducir el desorden en las salidas,\n","# es decir, para evitar que PyTorch rastree los gradientes de esas matrices\n","# durante operaciones que no requieren entrenamiento.\n"],"metadata":{"id":"KjDr1S3RQSJO","executionInfo":{"status":"ok","timestamp":1761017507761,"user_tz":180,"elapsed":5,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}}},"execution_count":104,"outputs":[]},{"cell_type":"code","source":["# calculamos los vectores de consulta (query), clave (key) y valor (value).\n","query_2 = x_2 @ W_query\n","key_2 = x_2 @ W_key\n","value_2 = x_2 @ W_value\n","print(query_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-M5wpIzYQ3h3","executionInfo":{"status":"ok","timestamp":1761017507797,"user_tz":180,"elapsed":28,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"ba6a5a20-95c4-4d0d-e705-2c65490bebbd"},"execution_count":105,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.4306, 1.4551])\n"]}]},{"cell_type":"code","source":["# obteniendo las keys y los valores via multiplicacion de matrix\n","keys = inputs @ W_key\n","values = inputs @ W_value\n","print(\"keys.shape: \", keys.shape)\n","print(\"values.shape: \", values.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4T0OTWDIuTMg","executionInfo":{"status":"ok","timestamp":1761017507823,"user_tz":180,"elapsed":20,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"98e6f7a0-1f4f-4871-d21d-fd2d1f0a9069"},"execution_count":106,"outputs":[{"output_type":"stream","name":"stdout","text":["keys.shape:  torch.Size([6, 2])\n","values.shape:  torch.Size([6, 2])\n"]}]},{"cell_type":"code","source":["# calcular los valores de atención w22\n","keys_2 = keys[1]\n","attn_score_22 = query_2.dot(keys_2)\n","print(attn_score_22)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TbKuA4C3ufIR","executionInfo":{"status":"ok","timestamp":1761017507901,"user_tz":180,"elapsed":74,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"0f03369f-cc44-4df1-d524-61381cc9e65d"},"execution_count":107,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.8524)\n"]}]},{"cell_type":"code","source":["attn_scores_2 = query_2 @ keys.T\n","print(attn_scores_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VVhfnH1BwyHN","executionInfo":{"status":"ok","timestamp":1761017507923,"user_tz":180,"elapsed":17,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"a37091f6-71e4-45e5-8ecd-900ca9355541"},"execution_count":108,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"]}]},{"cell_type":"code","source":["d_k = keys.shape[-1]\n","attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n","print(attn_weights_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rKOOV8Afxhk4","executionInfo":{"status":"ok","timestamp":1761017507950,"user_tz":180,"elapsed":22,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"6031b31d-1618-4157-c167-9d876026a102"},"execution_count":109,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"]}]},{"cell_type":"markdown","source":["### Justificación del mecanismo de atención por producto punto escalado\n","La razón por la cual se normalizan las puntuaciones de atención dividiéndolas por la dimensión de incrustación (embedding) es mejorar el rendimiento del entrenamiento evitando gradientes pequeños. Por ejemplo, al aumentar la dimensión de incrustación —que en modelos tipo GPT suele superar los 1.000— los productos punto pueden volverse muy grandes. Esto provoca que, al aplicar la función softmax, los gradientes durante la retropropagación se vuelvan muy pequeños.\n","A medida que los productos punto aumentan, la función softmax se comporta más como una función escalón, lo que lleva a gradientes cercanos a cero. Estos gradientes tan pequeños pueden ralentizar drásticamente el aprendizaje o incluso hacer que el entrenamiento se estanque.\n","Por eso se divide por la raíz cuadrada de la dimensión de incrustación: este ajuste es lo que da nombre al mecanismo de atención como atención por producto punto escalado (scaled-dot product attention).\n"],"metadata":{"id":"wHwNVgvPZiGA"}},{"cell_type":"code","source":["#  suma ponderada de los vectores de valor. En este caso, los pesos de atención\n","# actúan como factores de ponderación que determinan la\n","# importancia relativa de cada vector de valor.\n","\n","context_vec_2 = attn_weights_2 @ values\n","print(context_vec_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w-BrivIna_wL","executionInfo":{"status":"ok","timestamp":1761017507979,"user_tz":180,"elapsed":23,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"1c7856d6-c109-4759-859d-6c2d9b138973"},"execution_count":110,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.3061, 0.8210])\n"]}]},{"cell_type":"markdown","source":["### ¿Por qué se usan los términos query, key y value?\n","Los términos “clave” (key), “consulta” (query) y “valor” (value) en el contexto de los mecanismos de atención provienen del ámbito de la recuperación de información y las bases de datos, donde se utilizan conceptos similares para almacenar, buscar y recuperar información.\n","\n","Una consulta (query) es análoga a una búsqueda en una base de datos. Representa el elemento actual (por ejemplo, una palabra o token en una oración) en el que el modelo se enfoca o intenta comprender. La consulta se utiliza para explorar las demás partes de la secuencia de entrada y determinar cuánta atención se debe prestar a cada una.\n","La clave (key) es como una clave de base de datos usada para indexar y buscar. En el mecanismo de atención, cada elemento de la secuencia de entrada (por ejemplo, cada palabra en una oración) tiene una clave asociada. Estas claves se utilizan para comparar con la consulta.\n","El valor (value) en este contexto es similar al valor en un par clave-valor de una base de datos. Representa el contenido real o la representación de los elementos de entrada. Una vez que el modelo determina qué claves (y por lo tanto qué partes de la entrada) son más relevantes para la consulta (el elemento en foco), recupera los valores correspondientes."],"metadata":{"id":"ehxFKIincCv6"}},{"cell_type":"markdown","source":["### 3.4.2 Implmentando una clase en Python de  mecanismos de auto-atencion"],"metadata":{"id":"Vh0KpirWcPSC"}},{"cell_type":"code","source":["import torch.nn as nn\n","class SelfAttention_v1(nn.Module):\n","  def __init__(self, d_in, d_out):\n","    super().__init__()\n","    self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n","    self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n","    self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n","\n","  def forward(self, x):\n","    keys = x @ self.W_key\n","    queries = x @ self.W_query\n","    values = x @ self.W_value\n","    attn_scores = queries @ keys.T # Omega\n","    attn_weigths = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","    context_vec = attn_weigths @ values\n","    return context_vec"],"metadata":{"id":"HDkQKGAtcM5F","executionInfo":{"status":"ok","timestamp":1761017507990,"user_tz":180,"elapsed":6,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}}},"execution_count":111,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","sa_v1 = SelfAttention_v1(d_in, d_out)\n","print(sa_v1(inputs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cJzDk6ZAg0ij","executionInfo":{"status":"ok","timestamp":1761017508057,"user_tz":180,"elapsed":63,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"8a9bd5e0-dd88-44e6-a3a9-d2d6c1af059d"},"execution_count":112,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2996, 0.8053],\n","        [0.3061, 0.8210],\n","        [0.3058, 0.8203],\n","        [0.2948, 0.7939],\n","        [0.2927, 0.7891],\n","        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"]}]},{"cell_type":"code","source":["# A self-attention class using PyTorch’s Linea\n","\n","class SelfAttention_v2(nn.Module):\n","  def __init__(self, d_in, d_out, qkv_bias=False):\n","    super().__init__()\n","    self.W_query= nn.Linear(d_in, d_out, bias=qkv_bias)\n","    self.W_key= nn.Linear(d_in, d_out, bias=qkv_bias)\n","    self.W_value= nn.Linear(d_in, d_out, bias=qkv_bias)\n","\n","  def forward(self, x):\n","    keys = self.W_key(x)\n","    queries = self.W_query(x)\n","    values = self.W_value(x)\n","    attn_scores = queries @ keys.T\n","    attn_weigths = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","    context_vec = attn_weigths @ values\n","    return context_vec"],"metadata":{"id":"wRfd2i6ai7s1","executionInfo":{"status":"ok","timestamp":1761017508066,"user_tz":180,"elapsed":4,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}}},"execution_count":113,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(789)\n","sa_v2 = SelfAttention_v2(d_in, d_out)\n","print(sa_v2(inputs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g-gDZtCMkHQw","executionInfo":{"status":"ok","timestamp":1761017508089,"user_tz":180,"elapsed":19,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"dab42a77-c0d8-4211-8d21-06b1d62e8f77"},"execution_count":114,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-0.0739,  0.0713],\n","        [-0.0748,  0.0703],\n","        [-0.0749,  0.0702],\n","        [-0.0760,  0.0685],\n","        [-0.0763,  0.0679],\n","        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"]}]},{"cell_type":"markdown","source":["Ejercicio 3.1:\n","* Comparando SelfAttention_v1 y SelfAttention_v2\n","Ten en cuenta que nn.Linear en SelfAttention_v2 utiliza un esquema de inicialización de pesos diferente al de nn.Parameter(torch.rand(d_in, d_out)) usado en SelfAttention_v1, lo que provoca que ambos mecanismos produzcan resultados distintos. Para verificar que ambas implementaciones —SelfAttention_v1 y SelfAttention_v2— son similares en lo demás, podemos transferir las matrices de peso de un objeto SelfAttention_v2 a uno de SelfAttention_v1, de modo que ambos objetos produzcan los mismos resultados.\n","Tu tarea consiste en asignar correctamente los pesos de una instancia de SelfAttention_v2 a una instancia de SelfAttention_v1. Para hacerlo, necesitas comprender la relación entre los pesos en ambas versiones.\n","(Pista: nn.Linear almacena la matriz de pesos en forma transpuesta).\n","Después de realizar la asignación, deberías observar que ambas instancias generan las mismas salidas.\n"],"metadata":{"id":"pmzqdg5bt1m-"}},{"cell_type":"code","source":["torch.manual_seed(123)  # Semilla de la SelfAttention_v1\n","sa_v1 = SelfAttention_v1(d_in,d_out)\n","\n","torch.manual_seed(123)  # Semilla SelfAttention_v2 misma semilla para que pesos iniciales sean comparables\n","sa_v2 = SelfAttention_v2(d_in,d_out)\n"],"metadata":{"id":"N1zhyVZnt-Hd","executionInfo":{"status":"ok","timestamp":1761017508095,"user_tz":180,"elapsed":3,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}}},"execution_count":115,"outputs":[]},{"cell_type":"markdown","source":["nn.Linear almacena la matriz de pesos (weight) de forma transpuesta en comparación con cómo la definimos en SelfAttention_v1. Por lo tanto, al transferir los pesos, debemos transponer las matrices de pesos de sa_v2 antes de asignarlas a sa_v1. Los sesgos (bias), si existieran (qkv_bias=True), se transfieren directamente sin transponer."],"metadata":{"id":"58_GduPCvS6e"}},{"cell_type":"code","source":["sa_v1.W_query.data = sa_v2.W_query.weight.T\n","sa_v1.W_key.data = sa_v2.W_key.weight.T\n","sa_v1.W_value.data = sa_v2.W_value.weight.T"],"metadata":{"id":"GwrmX4-1u8GB","executionInfo":{"status":"ok","timestamp":1761017508105,"user_tz":180,"elapsed":5,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}}},"execution_count":116,"outputs":[]},{"cell_type":"code","source":["# Comparaciónde resultados\n","output_v1 = sa_v1(inputs)\n","output_v2 = sa_v2(inputs)\n","\n","print(\"Salida de SelfAttention_v1:\")\n","print(output_v1)\n","\n","print(\"\\nSalida de SelfAttention_v2:\")\n","print(output_v2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7V65i_y2vj07","executionInfo":{"status":"ok","timestamp":1761017508133,"user_tz":180,"elapsed":22,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"8fd602c9-36c6-43b5-b401-6a90f0a885c0"},"execution_count":117,"outputs":[{"output_type":"stream","name":"stdout","text":["Salida de SelfAttention_v1:\n","tensor([[-0.5337, -0.1051],\n","        [-0.5323, -0.1080],\n","        [-0.5323, -0.1079],\n","        [-0.5297, -0.1076],\n","        [-0.5311, -0.1066],\n","        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)\n","\n","Salida de SelfAttention_v2:\n","tensor([[-0.5337, -0.1051],\n","        [-0.5323, -0.1080],\n","        [-0.5323, -0.1079],\n","        [-0.5297, -0.1076],\n","        [-0.5311, -0.1066],\n","        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)\n"]}]},{"cell_type":"markdown","source":["### 3.5 Ocultando palabras futuras con atención causal\n","\n","En la atención causal, los pesos de atención por encima de la diagonal son enmascarados, lo que garantiza que, para cualquier entrada dada, el modelo de lenguaje (LLM) no pueda utilizar tokens futuros al calcular los vectores de contexto mediante los pesos de atención."],"metadata":{"id":"SkjSLjUmwOI_"}},{"cell_type":"code","source":["# implementar la máscara de atención causal\n","# puntciones e atencion (attenion scores)\n","\n","queries = sa_v2.W_query(inputs)\n","keys = sa_v2.W_key(inputs)\n","attn_scores = queries @ keys.T\n","attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","print(attn_weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JA_7mIxIZHl6","executionInfo":{"status":"ok","timestamp":1761017508211,"user_tz":180,"elapsed":80,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"cf7997b4-f869-4f8d-9e60-5a7d3d58a20f"},"execution_count":118,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1717, 0.1762, 0.1761, 0.1555, 0.1627, 0.1579],\n","        [0.1636, 0.1749, 0.1746, 0.1612, 0.1605, 0.1652],\n","        [0.1637, 0.1749, 0.1746, 0.1611, 0.1606, 0.1651],\n","        [0.1636, 0.1704, 0.1702, 0.1652, 0.1632, 0.1674],\n","        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.1639],\n","        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n","       grad_fn=<SoftmaxBackward0>)\n"]}]},{"cell_type":"code","source":["context_length = attn_scores.shape[0]\n","mask_simple = torch.tril(torch.ones(context_length, context_length))\n","print(mask_simple)\n","# mask with 0' s above diagonal"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kkv3458tUswi","executionInfo":{"status":"ok","timestamp":1761017508217,"user_tz":180,"elapsed":64,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"7bc30e9c-16b4-4143-acf9-b02e5069f713"},"execution_count":119,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 0., 0., 0., 0., 0.],\n","        [1., 1., 0., 0., 0., 0.],\n","        [1., 1., 1., 0., 0., 0.],\n","        [1., 1., 1., 1., 0., 0.],\n","        [1., 1., 1., 1., 1., 0.],\n","        [1., 1., 1., 1., 1., 1.]])\n"]}]},{"cell_type":"code","source":["masked_simple = attn_weights*mask_simple\n","print(masked_simple)\n","# multiplic las puntacionesde de atencion y ocn lamask"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPwBMzcbXnjp","executionInfo":{"status":"ok","timestamp":1761017508237,"user_tz":180,"elapsed":22,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"cea1220f-e03e-408b-e7bf-79d0ef880481"},"execution_count":120,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1717, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.1636, 0.1749, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.1637, 0.1749, 0.1746, 0.0000, 0.0000, 0.0000],\n","        [0.1636, 0.1704, 0.1702, 0.1652, 0.0000, 0.0000],\n","        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.0000],\n","        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n","       grad_fn=<MulBackward0>)\n"]}]},{"cell_type":"code","source":["#Volver a normalizar los pesos de atención para que la suma en\n","#cada fila sea igual a 1.\n","\n","row_sums = masked_simple.sum(dim=-1, keepdim=True)\n","masked_simple_norm=masked_simple/row_sums\n","print(masked_simple_norm)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sc9-33EVZCt_","executionInfo":{"status":"ok","timestamp":1761017508255,"user_tz":180,"elapsed":14,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"b4fb4372-1636-43fa-cc40-189560832aa1"},"execution_count":121,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n","        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n","        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n","        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n","       grad_fn=<DivBackward0>)\n"]}]},{"cell_type":"markdown","source":["Fugas de información\n","\n","Cuando aplicamos una máscara y luego renormalizamos los pesos de atención, podría parecer que la información de los tokens futuros (que pretendemos enmascarar) aún podría influir en el token actual, ya que sus valores forman parte del cálculo de la softmax.\n","Sin embargo, el punto clave es que al renormalizar los pesos de atención después del enmascaramiento, lo que realmente estamos haciendo es recalcular la softmax sobre un subconjunto más pequeño, ya que las posiciones enmascaradas no contribuyen al valor de la softmax.\n","La elegancia matemática de la función softmax radica en que, aunque inicialmente incluye todas las posiciones en el denominador, después del enmascaramiento y la renormalización, el efecto de las posiciones enmascaradas se anula: no aportan nada significativo al resultado de la softmax.\n","En términos más simples, después de aplicar la máscara y renormalizar, la distribución de los pesos de atención es como si se hubiera calculado solo entre las posiciones no enmascaradas desde el principio.\n","Esto garantiza que no haya fuga de información desde los tokens futuros (o cualquier otro token enmascarado), tal como se pretende.\n"],"metadata":{"id":"TjnFu0rMgqQ9"}},{"cell_type":"code","source":["# enmascaraminetso con -inf\n","mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n","masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n","print(masked)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kFfz8ZTZjzff","executionInfo":{"status":"ok","timestamp":1761017508273,"user_tz":180,"elapsed":14,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"35b84333-981f-43b1-d563-2e7cf0aadd0e"},"execution_count":122,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n","        [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n","        [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],\n","        [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],\n","        [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],\n","        [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n","       grad_fn=<MaskedFillBackward0>)\n"]}]},{"cell_type":"code","source":["attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n","print(attn_weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G8KsLD2QkV1e","executionInfo":{"status":"ok","timestamp":1761017508295,"user_tz":180,"elapsed":12,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"fe706edb-b023-4d9a-cb0f-4724eac3b987"},"execution_count":123,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n","        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n","        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n","        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n","       grad_fn=<SoftmaxBackward0>)\n"]}]},{"cell_type":"markdown","source":["### 3.5.2 Enmascarar pesos de atención adicionales con dropout\n","Dropout en aprendizaje profundo es una técnica en la que se ignoran aleatoriamente ciertas unidades de la capa oculta durante el entrenamiento, lo que equivale a “eliminarlas”. Este método ayuda a prevenir el sobreajuste al asegurar que el modelo no dependa excesivamente de ningún conjunto específico de unidades ocultas.\n"],"metadata":{"id":"zrkB__W2n0t9"}},{"cell_type":"code","source":["torch.manual_seed(123)\n","dropout = torch.nn.Dropout(0.5)\n","example = torch.ones(6,6)\n","print(dropout(example))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dAyWFme0oJcQ","executionInfo":{"status":"ok","timestamp":1761017508362,"user_tz":180,"elapsed":64,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"6902116e-4b61-42fd-b396-b723f953a7e3"},"execution_count":124,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[2., 2., 0., 2., 2., 0.],\n","        [0., 0., 0., 2., 0., 2.],\n","        [2., 2., 2., 2., 0., 2.],\n","        [0., 2., 2., 0., 0., 2.],\n","        [0., 2., 0., 2., 0., 2.],\n","        [0., 2., 2., 2., 2., 0.]])\n"]}]},{"cell_type":"code","source":["# Dropout a la matriz de pesos de atención\n","torch.manual_seed(123)\n","print(dropout(attn_weights))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bMU0FxlQogAx","executionInfo":{"status":"ok","timestamp":1761017508386,"user_tz":180,"elapsed":26,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"48be14ce-5b05-4896-8aec-8c17c4e7a3d3"},"execution_count":125,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.6380, 0.6816, 0.6804, 0.0000, 0.0000, 0.0000],\n","        [0.0000, 0.5090, 0.5085, 0.0000, 0.0000, 0.0000],\n","        [0.0000, 0.4120, 0.0000, 0.3869, 0.0000, 0.0000],\n","        [0.0000, 0.3418, 0.3413, 0.3308, 0.3249, 0.0000]],\n","       grad_fn=<MulBackward0>)\n"]}]},{"cell_type":"code","source":["batch = torch.stack((inputs,inputs), dim=0)\n","print(batch.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BMoewMgmp89u","executionInfo":{"status":"ok","timestamp":1761017508409,"user_tz":180,"elapsed":18,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"77ebb144-a9cf-4eca-99a1-15354c50479e"},"execution_count":126,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 6, 3])\n"]}]},{"cell_type":"markdown","source":["### 3.5.3 Implemenatando  la class causal attention"],"metadata":{"id":"3TB33B0TrAcV"}},{"cell_type":"code","source":["class CausalAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length,\n","                dropout, qkv_bias=False):\n","        super().__init__()\n","        self.d_out = d_out\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.dropout = nn.Dropout(dropout)               # Conpared to the previous SelfAttention_v1, add dropout layer\n","        self.register_buffer(\n","           'mask',\n","           torch.triu(torch.ones(context_length, context_length),\n","           diagonal=1)\n","        )\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape       # Se cambió la transposición\n","        keys = self.W_key(x)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","        attn_scores = queries @ keys.transpose(1, 2)\n","        attn_scores.masked_fill_(\n","            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n","        # `:num_tokens` para tener en cuenta los casos en que el número de tokens\n","        # en el lote es menor que el tamaño de contexto soportado\n","\n","        attn_weights = torch.softmax(\n","        attn_scores / keys.shape[-1]**0.5, dim=-1\n","        )\n","        attn_weights = self.dropout(attn_weights)\n","        context_vec = attn_weights @ values\n","        return context_vec\n"],"metadata":{"id":"_7NbA6h6rADm","executionInfo":{"status":"ok","timestamp":1761017508421,"user_tz":180,"elapsed":9,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}}},"execution_count":127,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","context_length = batch.shape[1]\n","ca = CausalAttention(d_in, d_out, context_length, 0.0)\n","context_vecs =  ca(batch)\n","print('context_vecs.shape: ', context_vecs.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4HvnfbLkq0qs","executionInfo":{"status":"ok","timestamp":1761017508480,"user_tz":180,"elapsed":56,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"280718fe-3e39-4a03-c5bc-f38aeba2fec7"},"execution_count":128,"outputs":[{"output_type":"stream","name":"stdout","text":["context_vecs.shape:  torch.Size([2, 6, 2])\n"]}]},{"cell_type":"markdown","source":["### 3.6 Extender la atención de una sola cabeza a atención de múltiples cabezas\n","El término “multi-cabeza” se refiere a dividir el mecanismo de atención en múltiples “cabezas”, cada una funcionando de manera independiente. En este contexto, un único módulo de atención causal puede considerarse como atención de una sola cabeza, donde solo hay un conjunto de pesos de atención procesando la entrada de forma secuencial.\n"],"metadata":{"id":"nKFRw7485dKd"}},{"cell_type":"markdown","source":["### 3.6.1 Apilar múltiples capas de atención de una sola cabeza\n","Implementar atención de múltiples cabezas implica crear múltiples instancias del mecanismo de autoatención."],"metadata":{"id":"1qC-9jl95q0J"}},{"cell_type":"code","source":["# Una clase contenedora para implementar atención de múltiples cabezas\n","class MultiHeadAttentionWrapper(nn.Module):\n","    def __init__(self, d_in, d_out, context_length,\n","                 dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        self.heads = nn.ModuleList(\n","            [CausalAttention(\n","                 d_in, d_out, context_length, dropout, qkv_bias\n","             )\n","             for _ in range(num_heads)]\n","        )\n","    def forward(self, x):\n","        return torch.cat([head(x) for head in self.heads], dim=-1)"],"metadata":{"id":"HjMqF5A7bas4","executionInfo":{"status":"ok","timestamp":1761017508482,"user_tz":180,"elapsed":6,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}}},"execution_count":129,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(123)\n","context_length = batch.shape[1]\n","d_in, d_out = 3 , 2\n","mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0,num_heads=2\n",")\n","context_vecs = mha(batch)\n","print(context_vecs)\n","\n","print('context_vecs.shape: ', context_vecs.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m1m6ZwFRiULC","executionInfo":{"status":"ok","timestamp":1761017508519,"user_tz":180,"elapsed":34,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"9d26c74d-824b-49c7-fe81-5290279e915a"},"execution_count":130,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n","         [-0.5874,  0.0058,  0.5891,  0.3257],\n","         [-0.6300, -0.0632,  0.6202,  0.3860],\n","         [-0.5675, -0.0843,  0.5478,  0.3589],\n","         [-0.5526, -0.0981,  0.5321,  0.3428],\n","         [-0.5299, -0.1081,  0.5077,  0.3493]],\n","\n","        [[-0.4519,  0.2216,  0.4772,  0.1063],\n","         [-0.5874,  0.0058,  0.5891,  0.3257],\n","         [-0.6300, -0.0632,  0.6202,  0.3860],\n","         [-0.5675, -0.0843,  0.5478,  0.3589],\n","         [-0.5526, -0.0981,  0.5321,  0.3428],\n","         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n","context_vecs.shape:  torch.Size([2, 6, 4])\n"]}]},{"cell_type":"markdown","source":["### Ejercico 3.2\n","Devolver vectores de embedding bidimensionales, debes ajustar los argumentos de entrada en la llamada a MultiHeadAttentionWrapper(..., num_heads=2) para que el vector de contexto resultante tenga dos dimensiones, en lugar de cuatro, sin cambiar la implementación de la clase.\n"],"metadata":{"id":"iIft25K3w25v"}},{"cell_type":"code","source":["d_out = 1\n","mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n","context_vecs = mha(batch)\n","print(context_vecs)\n","\n","print('context_vecs.shape: ', context_vecs.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fyDY7Pqgw2nq","executionInfo":{"status":"ok","timestamp":1761017508529,"user_tz":180,"elapsed":27,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"89aa81ac-6be6-4c7a-cde7-a49517e56e49"},"execution_count":131,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[0.0189, 0.2729],\n","         [0.2181, 0.3037],\n","         [0.2804, 0.3125],\n","         [0.2830, 0.2793],\n","         [0.2476, 0.2541],\n","         [0.2748, 0.2513]],\n","\n","        [[0.0189, 0.2729],\n","         [0.2181, 0.3037],\n","         [0.2804, 0.3125],\n","         [0.2830, 0.2793],\n","         [0.2476, 0.2541],\n","         [0.2748, 0.2513]]], grad_fn=<CatBackward0>)\n","context_vecs.shape:  torch.Size([2, 6, 2])\n"]}]},{"cell_type":"markdown","source":["### 3.6.2 Implementar atención de múltiples cabezas con división de pesos\n","En lugar de mantener dos clases separadas, MultiHeadAttentionWrapper y CausalAttention, podemos combinar estos conceptos en una sola clase llamada MultiHeadAttention. Además de fusionar el código de MultiHeadAttentionWrapper con el de CausalAttention, realizaremos algunas otras modificaciones para implementar la atención de múltiples cabezas de manera más eficiente\n"],"metadata":{"id":"TDIwR2V0010O"}},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        assert (d_out % num_heads == 0), \\\n","            \"d_out debe ser divisible por num_heads\"\n","\n","        self.d_out = d_out\n","        self.num_heads = num_heads\n","        self.head_dim = d_out // num_heads  # Reduce la dimensión de proyección para que coincida con la dimensión de salida deseada\n","\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)  # Capa lineal para combinar las salidas de las cabezas\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(torch.ones(context_length, context_length),\n","                       diagonal=1)\n","        )\n","\n","    def forward(self, x):\n","        b, num_tokens, d_in = x.shape\n","        # Como en `CausalAttention`, si el número de tokens excede `context_length`,\n","        # esto provocará errores en la creación de la máscara más abajo.\n","        # En la práctica, esto no es un problema ya que el LLM (capítulos 4–7) asegura que las entradas\n","        # no excedan `context_length` antes de llegar a este método forward.\n","\n","        keys = self.W_key(x)  # Forma: (b, num_tokens, d_out)\n","        queries = self.W_query(x)\n","        values = self.W_value(x)\n","\n","        # Dividimos implícitamente la matriz agregando una dimensión `num_heads`\n","        # Desenrollamos la última dimensión: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n","        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n","\n","        # Transposición: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n","        keys = keys.transpose(1, 2)\n","        queries = queries.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Calculamos la atención de producto punto escalado (también conocida como autoatención) con una máscara causal\n","        attn_scores = queries @ keys.transpose(2, 3)  # Producto punto para cada cabeza\n","\n","        # Máscara original truncada al número de tokens y convertida a booleano\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","\n","        # Usamos la máscara para rellenar los puntajes de atención\n","        attn_scores.masked_fill_(mask_bool, -torch.inf)\n","\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Forma: (b, num_tokens, num_heads, head_dim)\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","\n","        # Combinamos las cabezas, donde self.d_out = self.num_heads * self.head_dim\n","        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec)  # proyección opcional\n","\n","        return context_vec\n","\n","# Fijamos la semilla para reproducibilidad\n","torch.manual_seed(123)\n","\n","# Extraemos las dimensiones del lote\n","batch_size, context_length, d_in = batch.shape\n","d_out = 2\n","\n","# Creamos la instancia de atención multi-cabeza\n","mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n","\n","# Ejecutamos el modelo sobre el lote\n","context_vecs = mha(batch)\n","\n","# Mostramos los vectores de contexto y su forma\n","print(context_vecs)\n","print(\"context_vecs.shape:\", context_vecs.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wcH2bq4H1L4f","executionInfo":{"status":"ok","timestamp":1761017735079,"user_tz":180,"elapsed":41,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"14251a7f-4d50-48fe-f70e-880910153945"},"execution_count":132,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[0.3190, 0.4858],\n","         [0.2943, 0.3897],\n","         [0.2856, 0.3593],\n","         [0.2693, 0.3873],\n","         [0.2639, 0.3928],\n","         [0.2575, 0.4028]],\n","\n","        [[0.3190, 0.4858],\n","         [0.2943, 0.3897],\n","         [0.2856, 0.3593],\n","         [0.2693, 0.3873],\n","         [0.2639, 0.3928],\n","         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n","context_vecs.shape: torch.Size([2, 6, 2])\n"]}]},{"cell_type":"markdown","source":["### Ejercicio 3.3: Inicialización de módulos de atención del tamaño de GPT-2\n","Usando la clase MultiHeadAttention, inicializa un módulo de atención de múltiples cabezas que tenga el mismo número de cabezas de atención que el modelo GPT-2 más pequeño (12 cabezas de atención). Además, asegúrate de usar los tamaños de embedding de entrada y salida correspondientes al modelo GPT-2 (768 dimensiones). Ten en cuenta que el modelo GPT-2 más pequeño admite una longitud de contexto de 1.024 tokens.\n"],"metadata":{"id":"gkXl-k8b3jSm"}},{"cell_type":"code","source":["# Parámetros similares al GPT-2 pequeño\n","d_in = 768           # Tamaño de embedding de entrada\n","d_out = 768          # Tamaño de embedding de salida\n","context_length = 1024\n","num_heads = 12\n","dropout = 0.0        # Sin dropout para este ejemplo\n","\n","# Inicializar el módulo de atención multi-cabeza\n","mha = MultiHeadAttention(d_in, d_out, context_length, dropout, num_heads)\n","\n","# Simular un lote de entrada (por ejemplo, batch de 2 secuencias de 1024 tokens)\n","batch = torch.randn(2, context_length, d_in)\n","\n","# Ejecutar el módulo\n","context_vecs = mha(batch)\n","\n","# Mostrar resultados\n","print(context_vecs)\n","print(\"context_vecs.shape:\", context_vecs.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8IRuIXwu3WHW","executionInfo":{"status":"ok","timestamp":1761018212560,"user_tz":180,"elapsed":615,"user":{"displayName":"Gabriel Acevedo","userId":"07880318526769630073"}},"outputId":"7cd0cd51-be66-4a7c-86bb-fa64b761401b"},"execution_count":133,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[ 2.5606e-01, -5.9930e-02,  3.9847e-01,  ...,  2.4789e-01,\n","           1.8914e-01, -1.3538e-01],\n","         [ 2.7349e-01,  2.0641e-01,  1.9273e-01,  ...,  2.1027e-01,\n","           1.5678e-01, -4.5343e-01],\n","         [ 3.3144e-02,  1.0040e-01,  7.7156e-02,  ..., -3.8450e-02,\n","          -1.1032e-01, -4.1691e-01],\n","         ...,\n","         [-2.5707e-02, -6.9448e-03,  1.0252e-02,  ...,  3.4811e-02,\n","          -1.6675e-03, -3.0555e-02],\n","         [-3.2742e-02, -4.4450e-03, -7.9055e-04,  ...,  3.0958e-02,\n","          -8.3340e-03, -3.2058e-02],\n","         [-3.0148e-02, -1.3552e-03, -3.0732e-04,  ...,  2.4457e-02,\n","          -1.4528e-02, -3.3910e-02]],\n","\n","        [[ 3.2940e-01, -1.7189e-01, -4.2380e-01,  ...,  1.5147e-01,\n","          -3.5167e-01, -4.5434e-01],\n","         [-5.7646e-02,  8.6548e-02, -3.5733e-01,  ..., -1.3894e-01,\n","          -2.9612e-01, -1.3889e-01],\n","         [ 1.7297e-01,  8.9216e-02,  3.5174e-02,  ..., -1.9757e-01,\n","          -3.0677e-01, -1.6127e-01],\n","         ...,\n","         [-2.5912e-02, -2.0461e-02,  1.1347e-02,  ...,  3.5594e-02,\n","          -9.6283e-03, -2.0108e-02],\n","         [-1.8153e-02, -1.7910e-02,  1.6418e-02,  ...,  3.9557e-02,\n","          -5.9492e-03, -1.1958e-02],\n","         [-1.9344e-02, -2.1127e-02,  1.8900e-02,  ...,  2.8129e-02,\n","          -8.4774e-04, -2.6402e-02]]], grad_fn=<ViewBackward0>)\n","context_vecs.shape: torch.Size([2, 1024, 768])\n"]}]}]}